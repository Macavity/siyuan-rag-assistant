# 思源本地大语言模型集成插件

本插件实现了本地大语言模型（LLM）与思源的无缝集成，使用了强大的检索增强生成（RAG）技术。RAG 将您的笔记和可选的子文档作为检索到的上下文，以增强发送给 AI 的提示，使模型能够生成准确、上下文感知的响应，无需手动复制粘贴。这在您的知识管理环境中直接创建了一个注重隐私的离线 AI 体验。

## 演示

https://github.com/user-attachments/assets/eb5ad265-a083-4bd5-bcbf-b48a5654c475

## 功能特性

- 🤖 **AI 驱动的助手**：与由本地 Ollama LLM 驱动的 AI 助手进行对话
- 📄 **文档感知上下文**：自动使用当前打开的文档内容作为上下文
- 📚 **子文档支持**：可选择性地将同一文件夹中的所有子文档作为额外上下文
- 💬 **按文档的聊天历史**：为每个文档维护独立的对话历史
- ⚙️ **灵活配置**：自定义模型、温度和上下文设置

## 多文档任务推荐使用的 Ollama 模型

- Llama 3.1（8B 或 70B）：最先进的模型，非常适合深度总结、分析和评估
- Mistral 7B：高效且性能卓越，平衡了速度和准确性
- Granite 3.x (8B)：准确性强，效率高
- Phi-3 (3.8B)：轻量级但功能强大，非常适合资源受限的设置
- Gemma 7B：针对对话和总结任务进行了优化

示例：`ollama run llama3.1` - 下载 Llama 3.1 并在思源中可用

## 前置要求

- 已安装并运行 [Ollama](https://ollama.ai/)
- 在 Ollama 中至少下载一个 AI 模型（例如：`llama3.1`、`mistral`、`codellama`）

### 配置

1. 打开插件设置（点击侧边栏中的插件图标）
2. 配置您的 Ollama URL（默认：`http://localhost:11434`）
3. 选择您偏好的 AI 模型
4. 如需要，调整温度（推荐 0.1 以获得准确响应）
5. 启用"包含子文档"如果您希望 AI 考虑同一文件夹中的子文档

## 使用方法

1. 在思源中打开任何文档
2. 打开 RAG 助手面板
3. 向文档提问 - AI 将使用文档内容作为上下文
4. 您的对话历史按文档保存，但也可以删除

## 什么是检索增强生成（RAG）？

RAG 是一种 AI 技术，通过从您的笔记或外部来源检索相关文档并将其作为提示中的上下文，来改进大语言模型的输出。这种混合方法确保响应准确、最新，并基于您的特定数据，减少幻觉并提高可靠性。它在您的个人知识库中进行多文档总结、分析和评估任务时特别强大。

### 示例问题

- "这个文档是关于什么的？"
- "总结关键要点"
- "列出了哪些主要任务？"
- "列出此文档中的所有标题"
- "提到了哪些关于 X 的信息？"

## 设置

- **无上下文聊天**：启用后，对话将不使用文档上下文（通用 AI 聊天模式）
- **包含子文档**：启用后，AI 可以访问同一文件夹中的所有文档作为上下文
